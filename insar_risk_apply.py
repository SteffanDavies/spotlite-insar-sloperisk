# -*- coding: utf-8 -*-
"""
Created on Wed Dec 17 13:54:59 2025

@author: domin
"""


# --- FILE: insar_risk_apply.py ---
"""
InSAR Analysis Workflow: Execution Script
"""

import os
import pandas as pd
import matplotlib.pyplot as plt

# ---------------------------------------------------------
# 1. SETUP & IMPORT
# ---------------------------------------------------------
# This imports ALL functions and MODEL_MAP from your functions file.
# It is exactly like running source("insar_functions.R") in R.
from insar_risk import * # Set your working directory for this analysis
# Note: You can change this anytime
my_work_dir = r"C:\Users\domin\Documents\slope\risk"
os.chdir(my_work_dir)

pd.set_option('display.max_columns', None)

# OPTIONAL: Make models dynamic without touching the functions file
# Example: changing Random Forest to use 500 trees instead of 100
# MODEL_MAP['rf'] = RandomForestRegressor(n_estimators=500, random_state=123)

# This forces Pandas to display ALL columns, no matter how many there are
# pd.set_option('display.max_columns', None)

def risk_processing(pred_name, model_type='mlpe', centers=5, nstart=25, 
                    alpha=0.7, beta=0.3, WDir=None, 
                    original_csv="A13_vertical_2016_2023_clipped.csv", 
                    saveresults=None):
    """
    Analyzes displacement predictions to compute a spatial Risk Index based on 
    volatility and displacement magnitude.

    This function implements a multi-step risk assessment pipeline:
    1. Loads raw predictions generated by `perform_prediction`.
    2. Calculates 'Volatility' (stage differences over time).
    3. Performs PCA to reduce dimensions to two factors: Volatility vs. Displacement State.
    4. Clusters the PCA results using K-Means to identify distinct slope behaviors.
    5. Assigns a Risk Index (0-1) and categorical Risk Levels (Low to Very High) 
       based on the cluster characteristics (Slope Class) and the final displacement stage (Hazard).

    Parameters
    ----------
    pred_name : str or dict
        The input prediction data. Can be:
        - A string path to a saved joblib .pkl file (output from `perform_prediction`).
        - A dictionary object containing the predictions directly.
    model_type : str, optional (default='mlpe')
        The algorithm used for the predictions (e.g., 'xgboost', 'mlpe', 'rf'). 
        This is used to locate the correct column in the dataset (e.g., 'xgboost_T12D').
    centers : int, optional (default=5)
        The number of clusters (k) to use for K-Means. This determines the number 
        of Slope Classes (A, B, C, D, E...).
    nstart : int, optional (default=25)
        Number of random initializations for K-Means. Higher values ensure a more 
        stable clustering result.
    alpha : float, optional (default=0.7)
        Weighting factor for the 'Hazard' component (magnitude of movement) in the Risk Index.
    beta : float, optional (default=0.3)
        Weighting factor for the 'Slope Class' component (behavioral history) in the Risk Index.
    WDir : str, optional
        Path to the working directory. If provided, the script changes to this directory 
        before execution.
    original_csv : str, optional
        Filename of the original CSV containing spatial coordinates (latitude/longitude/PointID).
        Required to generate spatial outputs (GeoPackage).
    saveresults : str, optional
        The base filename to save results. If provided, the function saves:
        - {saveresults}.pkl : The full results dictionary.
        - {saveresults}_spatial.gpkg : A GeoPackage with spatial layers.
        - {saveresults}_pca_clust.tiff : A high-res plot of the clusters.

    Returns
    -------
    dict
        A dictionary containing all processing artifacts:
        - 'com12D' (pd.DataFrame): The main processed dataset with Risk Index and Categories.
        - 'area_risk' (pd.DataFrame): Aggregated max risk per PointID.
        - 'metrics' (dict): Clustering performance stats (TSS, WSS, BSS, Ratio).
        - 'cluster_centers' (pd.DataFrame): Coordinates of the cluster centroids.
        - 'pca_kmeans_result' (sklearn.KMeans): The trained clustering model.
        - 'com12D_sf', 'area_risk_sf' (GeoDataFrame): Spatial dataframes (if coords found).

    Example
    -------
    >>> results = risk_processing(
            pred_name="risk.pred.pkl",
            model_type="xgboost",
            centers=5,
            alpha=0.7,
            original_csv="site_data.csv",
            saveresults="site_risk_analysis"
        )
    """
    # ... (function body) ...



# Assuming you have imported perform_prediction

# 1. Run the Prediction Model
# This generates the raw stage predictions based on your model.
predict_movement = perform_prediction(
    model_name="Smovement_I22_py.model",      # Your trained ML model
    new_dataset_name="ver_A13.csv",           # New data to predict on
    intervals=[12],
    pred_times=[12],
    WDir=r"C:\Users\domin\Documents\slope\risk",
    saveresults='risk'                   # SAVES: risk.pred.pkl
)

print("Prediction complete. File saved as 'risk.pred.pkl'")







# ---------------------------------------------------------
# 1. SETUP PARAMETERS
# ---------------------------------------------------------
working_dir = r"C:\Users\domin\Documents\slope\risk"
model_path  = "risk.pred.pkl"
csv_path    = "A13_vertical_2016_2023_clipped.csv"

my_alpha     = 0.7
my_beta      = 0.3
num_clusters = 5
model_algo   = 'xgboost'

# ---------------------------------------------------------
# 3. RUN THE RISK PROCESSING
# ---------------------------------------------------------
results = risk_processing(
    pred_name=model_path,
    model_type=model_algo,
    centers=num_clusters,
    nstart=25,
    alpha=my_alpha,
    beta=my_beta,
    WDir=working_dir,
    original_csv=csv_path,
    saveresults="final_risk_v1"
)


# ## another way:# 2. Define Parameters
# working_dir = r"C:\Users\domin\Documents\slope\risk"
# model_path  = "risk.pred.pkl"               # This matches the saveresults from Step 1
# csv_path    = "A13_vertical_2016_2023_clipped.csv"

# # 3. Run Risk Processing
# results = risk_processing(
#     pred_name=model_path,
#     model_type='xgboost',      # Ensure this matches the algo used in Step 1
#     centers=5,
#     nstart=25,
#     alpha=0.7,                 # 70% weight on current movement magnitude
#     beta=0.3,                  # 30% weight on historical behavior (slope class)
#     WDir=working_dir,
#     original_csv=csv_path,
#     saveresults="final_risk_v1" # SAVES: final_risk_v1.pkl, final_risk_v1_spatial.gpkg, plots...
# )


# ---------------------------------------------------------
# 4. INSPECT THE RESULTS
# ---------------------------------------------------------
# The function returns a dictionary, so you access parts like this:

# A. Check the Risk Table
print("\n--- Risk Table (First 5 rows) ---")
print(results['com12D'][['PointID', 'RiskIndex', 'RiskCategory']].head())

# B. Check the Aggregated Area Risk
print("\n--- Max Risk per Point (First 5 rows) ---")
print(results['area_risk'][['PointID', 'MaxRiskIndex', 'MaxRiskCategory']].head())

# C. Check Cluster Centers (Returned as DataFrame)
print("\n--- PCA Cluster Centers ---")
print(results['cluster_centers'])

# D. Check Clustering Performance Metrics
print("\n--- Clustering Performance Metrics ---")
metrics = results['metrics']
print(f"Total Sum of Squares (TSS): {metrics['tss']:.2f}")
print(f"Within-Cluster SS  (WSS): {metrics['wss']:.2f}")
print(f"Between-Cluster SS (BSS): {metrics['bss']:.2f}")
print(f"BSS / TSS Ratio:          {metrics['bss_tss_ratio']:.2f}%")

# E. Check Cluster Sizes and Proportions
print("\n--- Cluster Sizes ---")
print(results['cluster_counts'])


### Step 3: How to Load Saved Results Later
### If you close Python and come back tomorrow, you don't need to rerun everything. 
### You can simply load the results.

import joblib
import geopandas as gpd

# --- Method 1: Load the Python Dictionary (.pkl) ---
# This restores the exact variable 'results' from Step 2
loaded_results = joblib.load("final_risk_v1.pkl")

# Access data just like before:
print(loaded_results['metrics']['bss_tss_ratio'])
print(loaded_results['com12D'].head())


# --- Method 2: Load the Spatial Data for GIS (.gpkg) ---
# If you want to analyze the map data or load it into QGIS/ArcGIS
gdf_points = gpd.read_file("final_risk_v1_spatial.gpkg", layer='com12D_sf')
gdf_area   = gpd.read_file("final_risk_v1_spatial.gpkg", layer='area_risk_sf')

print(gdf_area.head())